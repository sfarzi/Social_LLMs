# -*- coding: utf-8 -*-
"""Train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k8WprvAMnHmKK63QsA9BT3RYPI9UrGEH
"""

# Hyperparameters
BARTtokenizer = AutoTokenizer.from_pretrained(
    "facebook/bart-base",
    lowercase = True
    )
BARTmodel = BartForConditionalGeneration.from_pretrained(
    "facebook/bart-base",
    output_hidden_states=False,
    forced_bos_token_id=0
    )
hparams_Main = {
    'embed_size' : BARTmodel.config.d_model,
    'DO_r' : 0.1,
    'lr_RQE' : 2e-5,
    'lr_SUM' : 2e-5,
    'lr_TG' : 2e-5,
    'lr_Encoder' : 2e-5,
    'lr_Decoder' : 2e-5,
    'Dimensions' : [48, 24, 2]
    }
Main_Arch_Obj = Main_Architecture(
    BARTmodel,
    BARTmodel.config,
    hparams_Main
    )

vocab_size = BARTmodel.config.vocab_size
label_smoothing=  0.1
CE_Weight, BS_Weight, RL_Weight = 1 ,0, 0
CE_Loss = nn.CrossEntropyLoss(label_smoothing=label_smoothing)
Mixed_Loss = MixedLoss(
    label_smoothing=label_smoothing,
    CE_Weight=CE_Weight,
    BS_Weight=BS_Weight,
    RL_Weight=RL_Weight
    )

max_epochs = 35
batch_size = 16
split_ratio=   [0.8, 0.1, 0.1]


hparams_MyModel = {
    'Model': Main_Arch_Obj,
    'tokenizer': BARTtokenizer,
    'CELoss': CE_Loss,
    'MixedLoss': Mixed_Loss,
    'FreezeLayers':[[0, 0, 0, 1, 1, 1],
                    [1, 1, 1, 1, 1, 1]],
    'Coefficient': {'RQE': 0.7, 'SUM':0.2, 'TG':0.1,
                    'SUM2':1, 'TG2':1,
                    'RQE3':1, 'SUM3':1, 'TG3':1},
    'max_epochs': max_epochs,
    'warmup': 0.0,
    'epsilon': 1e-8,
    }


Result_path = "/content/drive/MyDrive/ReQuEST/Results/"
print(
    "Maximum position embeddings: ",
    BARTmodel.config.max_position_embeddings
    )
print("Size of embeddings: " , BARTmodel.config.d_model)



# Load data
Data_Path = '/content/drive/MyDrive/ReQuEST/data/CQAD_ReQuEST'
MyData = pd.read_pickle(Data_Path)

MyData = Preprocess_Data(MyData)
MyData.reset_index(drop=True, inplace=True)
display(MyData)


# Create data modules
DataModule = CustomDataModule(
    MyData, BARTtokenizer, BARTmodel.config.pad_token_id,
    batch_size=batch_size, split_ratio=split_ratio
)

DataModule.setup()
num_train_batches = len(DataModule.train_dataloader())
hparams_MyModel['num_train_batches'] = num_train_batches


# Model compile
MyModel = LitReQuEST(hparams_MyModel)
print(MyModel)

logger = TensorBoardLogger(
    "/content/drive/MyDrive/ReQuEST/logs",
    name="ReQuEST_Logs"
    )


# Training phase
trainer = pl.Trainer(
    logger = logger,
    max_epochs = max_epochs,
    num_sanity_val_steps = -1,
    callbacks = [OverrideEpochStepCallback()],
    default_root_dir="/content/drive/MyDrive/ReQuEST/Checkpoints",
    )

notebook.start("--logdir /content/drive/MyDrive/ReQuEST/logs/ReQuEST_Logs/")

trainer.fit(
    MyModel,
    datamodule=DataModule,
    )

trainer.save_checkpoint(
    '/content/drive/MyDrive/ReQuEST/Checkpoints/MyModel_checkpoints.ckpt'
    )


# Test phase
result = trainer.test(MyModel, datamodule=DataModule)