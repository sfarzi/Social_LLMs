# -*- coding: utf-8 -*-
"""Libraries.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k8WprvAMnHmKK63QsA9BT3RYPI9UrGEH
"""

!pip install transformers
!pip install torchsummary
!pip install torchviz
!pip install torchmetrics
!pip install rouge
!pip install pytorch_lightning
!pip install torchvision
!pip install tensorboard

import nltk
import re
import json
import time
import sklearn
import sys
import datetime
import copy
import string
import transformers
import torch
import warnings
import logging
import math

import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

from sklearn import metrics
from typing import Tuple
from datetime import timedelta
from statistics import mean
from IPython.display import clear_output

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

from transformers import logging
from transformers import get_linear_schedule_with_warmup
from transformers import AutoTokenizer, BartForConditionalGeneration
from transformers import BartModel, BartPretrainedModel, BartConfig
from transformers.modeling_outputs import Seq2SeqLMOutput

from torch import autograd
from torch.utils.data import DataLoader, random_split
from torch.utils.data import Dataset, SequentialSampler
from torchsummary import summary
from torchviz import make_dot

from torchmetrics import MetricCollection
from torchmetrics.text.rouge import ROUGEScore
from torchmetrics.classification import Accuracy, F1Score
from torchmetrics.text import BERTScore

from rouge import Rouge
import pytorch_lightning as pl
from pytorch_lightning.loggers import TensorBoardLogger
from pytorch_lightning.callbacks import Callback
from tensorboard import notebook

from matplotlib.rcsetup import validate_backend
from os import truncate

nltk.download('punkt')
nltk.download('stopwords')
logging.set_verbosity_error()


# ========================= Torch AND GPU =========================
if torch.cuda.is_available():
    device = torch.device("cuda")
    print('There are %d GPU(s) available.' % torch.cuda.device_count())
    print('We will use the GPU:', torch.cuda.get_device_name(0))
else:
    print('No GPU available, using the CPU instead.')
    device = torch.device("cpu")

warnings.filterwarnings('ignore')