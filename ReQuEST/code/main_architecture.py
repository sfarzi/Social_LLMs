# -*- coding: utf-8 -*-
"""Main_architecture.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k8WprvAMnHmKK63QsA9BT3RYPI9UrGEH
"""

class Main_Architecture(BartPretrainedModel):
  def __init__(self, BART_obj, config: BartConfig, hparams):
    super(Main_Architecture, self).__init__(config)

    self.learning_rate_Encoder = hparams['lr_Encoder']
    self.learning_rate_Decoder = hparams['lr_Decoder']

    self.learning_rate_RQE = hparams['lr_RQE']
    self.learning_rate_SUM = hparams['lr_SUM']
    self.learning_rate_TG = hparams['lr_TG']

    self.RQE = NN_model_RQE_Obj = NN_model_RQE(
        hparams['embed_size'],
        hparams['Dimensions'],
        hparams['DO_r'],
        hparams['lr_RQE']
        )
    self.SUM = copy.deepcopy(BART_obj.model)
    self.TG = copy.deepcopy(BART_obj.model)

    self.SUM.shared = BART_obj.model.shared
    self.SUM.encoder = BART_obj.model.encoder
    self.SUM.decoder.embed_tokens = BART_obj.model.decoder.embed_tokens
    self.SUM.decoder.embed_positions = BART_obj.model.decoder.embed_positions
    self.SUM.decoder.layernorm_embedding = BART_obj.model.decoder.layernorm_embedding
    self.SUM.decoder.layers[0] = BART_obj.model.decoder.layers[0]
    self.SUM.decoder.layers[1] = BART_obj.model.decoder.layers[1]
    self.SUM.decoder.layers[2] = BART_obj.model.decoder.layers[2]
    self.SUM_lm_head = copy.deepcopy(BART_obj.lm_head)

    self.TG.shared = self.SUM.shared
    self.TG.encoder = self.SUM.encoder
    self.TG.decoder.embed_tokens = self.SUM.decoder.embed_tokens
    self.TG.decoder.embed_positions = self.SUM.decoder.embed_positions
    self.TG.decoder.layernorm_embedding = self.SUM.decoder.layernorm_embedding
    self.TG.decoder.layers[0] = self.SUM.decoder.layers[0]
    self.TG.decoder.layers[1] = self.SUM.decoder.layers[1]
    self.TG.decoder.layers[2] = self.SUM.decoder.layers[2]
    self.TG_lm_head = copy.deepcopy(BART_obj.lm_head)

    self.register_buffer("final_logits_bias",
                         torch.zeros((1, self.SUM.shared.num_embeddings)))


  def forward(self,
              EncoderRQE_input_ids = None,
              EncoderRQE_attention = None,
              EncoderSUM_input_ids = None,
              EncoderSUM_attention_mask = None,
              EncoderTG_input_ids = None,
              EncoderTG_attention_mask = None,
              DecoderSUM_input_ids = None,
              DecoderSUM_attention_mask = None,
              DecoderTG_input_ids = None,
              DecoderTG_attention_mask = None,
              output_attentions = None,
              output_hidden_states = None,
              encoder_outputs = None,
              SUM_labels = None,
              TG_labels = None,
              past_key_values = None,
              head_mask = None,
              decoder_head_mask = None,
              cross_attn_head_mask = None,
              inputs_embeds = None,
              decoder_inputs_embeds = None,
              use_cache = None,
              return_dict = None,
              decoder_task = None
              ):

    return_dict = return_dict if return_dict is not None else self.config.use_return_dict

    # ****************** For Summarization *****************
    if EncoderSUM_input_ids is not None or\
        DecoderSUM_input_ids is not None or\
          EncoderSUM_attention_mask is not None or\
            DecoderSUM_attention_mask is not None:

      if (SUM_labels is not None):
        DecoderSUM_input_ids = self.shift_tokens_right(
            SUM_labels,
            self.SUM.config.pad_token_id,
            self.SUM.config.decoder_start_token_id
          )

      # ================= BART Model Layer =================
      BARTOutputs_SUM = self.SUM(
          input_ids = EncoderSUM_input_ids,
          attention_mask = EncoderSUM_attention_mask,
          decoder_input_ids = DecoderSUM_input_ids,
          decoder_attention_mask = DecoderSUM_attention_mask,
          encoder_outputs = encoder_outputs,
          head_mask = head_mask,
          decoder_head_mask = decoder_head_mask,
          cross_attn_head_mask = cross_attn_head_mask,
          past_key_values = past_key_values,
          inputs_embeds = inputs_embeds,
          decoder_inputs_embeds = decoder_inputs_embeds,
          use_cache = use_cache,
          output_attentions = output_attentions,
          output_hidden_states = True,
          return_dict = True
        )
      # ================ Summarization Head ================
      Last_HS_SUM = BARTOutputs_SUM[0]
      lm_logits_sum = self.SUM_lm_head(Last_HS_SUM) + self.final_logits_bias
    # ****************** End Summarization *****************



    # ********************** For RQE ***********************
    if (EncoderRQE_input_ids != None):
      # ================= BART Model Layer =================
      BARTOutputs_RQE = self.SUM(
          input_ids = EncoderRQE_input_ids,
          attention_mask = EncoderRQE_attention
        )
      Last_HS_RQE = BARTOutputs_RQE.encoder_last_hidden_state
      # ==================== RQE Head ======================
      decoder_last_embd = Last_HS_RQE[:, 0, :]
      Predicted_label = self.RQE(decoder_last_embd)
    # ********************** End RQE ***********************



    # ***************** For Tag Genertaion *****************
    if EncoderTG_input_ids is not None or\
        DecoderTG_input_ids is not None or\
          EncoderTG_attention_mask is not None or\
            DecoderTG_attention_mask is not None:

      if (TG_labels is not None):
        DecoderTG_input_ids = self.shift_tokens_right(
            TG_labels,
            self.TG.config.pad_token_id,
            self.TG.config.decoder_start_token_id
          )

      # ================= BART Model Layer =================
      BARTOutputs_TG = self.TG(
          input_ids = EncoderTG_input_ids,
          attention_mask = EncoderTG_attention_mask,
          decoder_input_ids = DecoderTG_input_ids,
          decoder_attention_mask = DecoderTG_attention_mask,
          encoder_outputs = encoder_outputs,
          head_mask = head_mask,
          decoder_head_mask = decoder_head_mask,
          cross_attn_head_mask = cross_attn_head_mask,
          past_key_values = past_key_values,
          inputs_embeds = inputs_embeds,
          decoder_inputs_embeds = decoder_inputs_embeds,
          use_cache = use_cache,
          output_attentions = output_attentions,
          output_hidden_states = True,
          return_dict = True
          )
      # ===================== TG Head ======================
      Last_HS_TG = BARTOutputs_TG.last_hidden_state
      lm_logits_TG = self.TG_lm_head(Last_HS_TG) + self.final_logits_bias
    # ***************** End Tag Genertaion *****************


    if not return_dict:
      if (EncoderRQE_input_ids == None and EncoderTG_input_ids == None):
        return lm_logits_sum
      elif (EncoderSUM_input_ids == None and EncoderTG_input_ids == None):
        return Predicted_label
      elif (EncoderSUM_input_ids == None and EncoderRQE_input_ids == None):
        return lm_logits_TG
      else:
        return lm_logits_sum, lm_logits_TG, Predicted_label
    elif return_dict == True and decoder_task == 'S':
      return Seq2SeqLMOutput(
        loss = 0,
        logits = lm_logits_sum,
        past_key_values = BARTOutputs_SUM.past_key_values,
        decoder_hidden_states = BARTOutputs_SUM.decoder_hidden_states,
        decoder_attentions = BARTOutputs_SUM.decoder_attentions,
        cross_attentions = BARTOutputs_SUM.cross_attentions,
        encoder_last_hidden_state = BARTOutputs_SUM.encoder_last_hidden_state,
        encoder_hidden_states = BARTOutputs_SUM.encoder_hidden_states,
        encoder_attentions = BARTOutputs_SUM.encoder_attentions,
        )
    elif return_dict == True and decoder_task == 'T':
      return Seq2SeqLMOutput(
        loss = 0,
        logits = lm_logits_TG,
        past_key_values = BARTOutputs_TG.past_key_values,
        decoder_hidden_states = BARTOutputs_TG.decoder_hidden_states,
        decoder_attentions = BARTOutputs_TG.decoder_attentions,
        cross_attentions = BARTOutputs_TG.cross_attentions,
        encoder_last_hidden_state = BARTOutputs_TG.encoder_last_hidden_state,
        encoder_hidden_states = BARTOutputs_TG.encoder_hidden_states,
        encoder_attentions = BARTOutputs_TG.encoder_attentions,
        )


  def resize_token_embeddings(self, new_num_tokens: int) -> nn.Embedding:
    new_embeddings = super().resize_token_embeddings(new_num_tokens)
    self._resize_final_logits_bias(new_num_tokens)
    return new_embeddings


  def _resize_final_logits_bias(self, new_num_tokens: int) -> None:
    old_num_tokens = self.final_logits_bias.shape[-1]
    if new_num_tokens <= old_num_tokens:
        new_bias = self.final_logits_bias[:, :new_num_tokens]
    else:
        extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens),
                                 device=self.final_logits_bias.device)
        new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)
    self.register_buffer("final_logits_bias", new_bias)


  def get_output_embeddings(self, decoder_task):
    if decoder_task == 'T':
      return self.TG_lm_head
    elif decoder_task == 'S':
      return self.SUM_lm_head


  def set_output_embeddings(self, new_embeddings, decoder_task):
    if decoder_task == 'T':
      self.TG_lm_head = new_embeddings
    elif decoder_task == 'S':
      self.SUM_lm_head = new_embeddings


  def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):
    return self.shift_tokens_right(
        labels,
        self.config.pad_token_id,
        self.config.decoder_start_token_id
        )


  def get_encoder(self):
    return self.SUM.get_encoder()


  def get_decoder(self, decoder_task):
    if decoder_task == 'S':
      return self.SUM.get_decoder()
    elif decoder_task == 'T':
      return self.TG.get_decoder()

  def _reorder_cache(framework, past_key_values, beam_idx):
    reordered_past = ()
    for layer_past in past_key_values:
      reordered_past += (
          tuple(past_state.index_select(0, beam_idx) for past_state in layer_past[:2]) + layer_past[2:],
      )
    return reordered_past


  def prepare_inputs_for_generation(self,
                                    decoder_input_ids,
                                    past_key_values=None,
                                    attention_mask=None,
                                    decoder_attention_mask=None,
                                    head_mask=None,
                                    decoder_head_mask=None,
                                    cross_attn_head_mask=None,
                                    use_cache=None,
                                    encoder_outputs=None,
                                    decoder_task = None,
                                    **kwargs):
    if (decoder_task == 'S'):
      if past_key_values is not None:
        decoder_input_ids = decoder_input_ids[:, -1:]
      return {
          "EncoderSUM_input_ids": None,
          "DecoderSUM_input_ids": decoder_input_ids,
          "encoder_outputs": encoder_outputs,
          "past_key_values": past_key_values,
          "EncoderSUM_attention_mask": attention_mask,
          "DecoderSUM_attention_mask": decoder_attention_mask,
          "head_mask": head_mask,
          "decoder_head_mask": decoder_head_mask,
          "cross_attn_head_mask": cross_attn_head_mask,
          "use_cache": use_cache,
          "decoder_task" : decoder_task
      }
    elif (decoder_task == 'T'):
      if past_key_values is not None:
        decoder_input_ids = decoder_input_ids[:, -1:]
      return {
          "EncoderTG_input_ids": None,
          "DecoderTG_input_ids": decoder_input_ids,
          "encoder_outputs": encoder_outputs,
          "past_key_values": past_key_values,
          "EncoderTG_attention_mask": attention_mask,
          "DecoderTG_attention_mask": decoder_attention_mask,
          "head_mask": head_mask,
          "decoder_head_mask": decoder_head_mask,
          "cross_attn_head_mask": cross_attn_head_mask,
          "use_cache": use_cache,
          "decoder_task" : decoder_task
      }


  def SUM_generate2(self,
                    tokenizer,
                    input_ids,
                    _min_length=5,
                    _max_length=10,
                    _num_beams=3,
                    _no_repeat_ngram_size=3
                    ):
    task = 'S'
    Summary_input_ids = super(Main_Architecture, self).generate(
        input_ids,
        max_length = _max_length,
        min_length = _min_length,
        num_beams = _num_beams,
        no_repeat_ngram_size = _no_repeat_ngram_size,
        decoder_task = task
        )
    Summary = tokenizer.batch_decode(
        Summary_input_ids,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False
        )
    return Summary


  def TG_generate2(self,
                   tokenizer,
                   input_ids,
                   _min_length=5,
                   _max_length=10,
                   _num_beams=3,
                   _no_repeat_ngram_size=3):
    task = 'T'
    Tag_input_ids = super(Main_Architecture, self).generate(
        input_ids,
        max_length = _max_length,
        min_length = _min_length,
        num_beams = _num_beams,
        no_repeat_ngram_size = _no_repeat_ngram_size,
        decoder_task = task
        )
    Tags = tokenizer.batch_decode(
        Tag_input_ids,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False
        )
    return Tags


  def RQE_predict(self, input_ids, attention_masks):
    logits = self.forward(
        EncoderRQE_input_ids = input_ids,
        EncoderRQE_attention = attention_masks,
        return_dict = False
        )
    Pr_Scores, Pr_labels = torch.max(logits, axis=1)
    return [Pr_Scores, Pr_labels]


  def Freeze_Parameters(self, FoN_Dec, FoN_Enc):
    # ============ BART Shared Layer ============
    for param in self.TG.shared.parameters():
      param.requires_grad = False
    for param in self.SUM.shared.parameters():
      param.requires_grad = False

    # =========== BART Decoder Layers ===========
    for i, FON in enumerate(FoN_Dec):
      if FON == 0:
        for param in self.TG.decoder.layers[i].parameters():
          param.requires_grad = False
        for param in self.SUM.decoder.layers[i].parameters():
          param.requires_grad = False

      if i==0 and FON==0:
        for param in self.TG.decoder.embed_positions.parameters():
          param.requires_grad = False
        for param in self.TG.decoder.embed_tokens.parameters():
          param.requires_grad = False
        for param in self.SUM.decoder.embed_positions.parameters():
          param.requires_grad = False
        for param in self.SUM.decoder.embed_tokens.parameters():
          param.requires_grad = False

      if i==5 and FON==0:
        for param in self.TG.decoder.layernorm_embedding.parameters():
          param.requires_grad = False
        for param in self.SUM.decoder.layernorm_embedding.parameters():
          param.requires_grad = False

    # =========== BART Encoder Layers ===========
    for i, FON in enumerate(FoN_Enc):
      if FON == 0:
        for param in self.SUM.encoder.layers[i].parameters():
          param.requires_grad = False
        for param in self.TG.encoder.layers[i].parameters():
          param.requires_grad = False

      if i==0 and FON==0:
        for param in self.TG.encoder.embed_positions.parameters():
          param.requires_grad = False
        for param in self.TG.encoder.embed_tokens.parameters():
          param.requires_grad = False
        for param in self.SUM.encoder.embed_positions.parameters():
          param.requires_grad = False
        for param in self.SUM.encoder.embed_tokens.parameters():
          param.requires_grad = False

      if i==5 and FON==0:
        for param in self.TG.encoder.layernorm_embedding.parameters():
          param.requires_grad = False
        for param in self.SUM.encoder.layernorm_embedding.parameters():
          param.requires_grad = False


  def shift_tokens_right(self,
                         input_ids,
                         pad_token_id,
                         decoder_start_token_id):
    shifted_input_ids = input_ids.new_zeros(input_ids.shape)
    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()
    shifted_input_ids[:, 0] = decoder_start_token_id
    if pad_token_id is None:
        raise ValueError("self.model.config.pad_token_id has to be defined.")
    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)
    return shifted_input_ids